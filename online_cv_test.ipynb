{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "online_cv_test.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wE2xUX2aXeg_",
        "QJUe6cbKXYw5",
        "UDVXqIc0XURI"
      ],
      "authorship_tag": "ABX9TyNFKuMIVOY1ZxSwgZ2jBDoD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomJiX/ML_start/blob/main/online_cv_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjDuBrExXrf5"
      },
      "source": [
        "#Package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqbdutP3X_WY"
      },
      "source": [
        "!pip install face_recognition"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QMf5_YpWzr9"
      },
      "source": [
        "#Package\n",
        "import face_recognition\n",
        "import cv2    \n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import sys\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wmgymEljIv4"
      },
      "source": [
        "#Data import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "669ONJx1jI9p"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlSTGnSuXogY"
      },
      "source": [
        "#Is mouth open"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dynn5J2rW6H6"
      },
      "source": [
        "# Mouth open/close\n",
        "def get_lip_height(lip):\n",
        "    sum=0\n",
        "    for i in [2,3,4]:\n",
        "        # distance between two near points up and down\n",
        "        distance = math.sqrt( (lip[i][0] - lip[12-i][0])**2 +\n",
        "                              (lip[i][1] - lip[12-i][1])**2   )\n",
        "        sum += distance\n",
        "    return sum / 3\n",
        "\n",
        "def get_mouth_height(top_lip, bottom_lip):\n",
        "    sum=0\n",
        "    for i in [8,9,10]:\n",
        "        # distance between two near points up and down\n",
        "        distance = math.sqrt( (top_lip[i][0] - bottom_lip[18-i][0])**2 + \n",
        "                              (top_lip[i][1] - bottom_lip[18-i][1])**2   )\n",
        "        sum += distance\n",
        "    return sum / 3\n",
        "\n",
        "def is_mouth_open(face_landmarks):\n",
        "    top_lip = face_landmarks['top_lip']\n",
        "    bottom_lip = face_landmarks['bottom_lip']\n",
        "\n",
        "    top_lip_height = get_lip_height(top_lip)\n",
        "    bottom_lip_height = get_lip_height(bottom_lip)\n",
        "    mouth_height = get_mouth_height(top_lip, bottom_lip)\n",
        "\n",
        "    # if mouth is open more than lip height * ratio, return true.\n",
        "    ratio = 0.5\n",
        "    print('top_lip_height: %.2f, bottom_lip_height: %.2f, mouth_height: %.2f, min*ratio: %.2f'\n",
        "          % (top_lip_height,bottom_lip_height,mouth_height, min(top_lip_height, bottom_lip_height) * ratio))\n",
        "\n",
        "    if mouth_height > min(top_lip_height, bottom_lip_height) * ratio:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def mouth_open_close(filename=\"\"):\n",
        "    # Get a reference to webcam #0 (the default one)\n",
        "    if len(filename)==0:\n",
        "      video_capture = cv2.VideoCapture(2)\n",
        "    else :\n",
        "      video_capture = cv2.VideoCapture(filename)\n",
        "    video_capture.set(3,640)\n",
        "    video_capture.set(4,480)\n",
        "    # Load a sample picture and learn how to recognize it.\n",
        "\n",
        "    while True:\n",
        "        # Grab a single frame of video\n",
        "        ret, frame = video_capture.read()\n",
        "        frame=cv2.cvtColor(frame,cv2.COLOR_RGB2GRAY)\n",
        "        frame=cv2.flip(frame, 1)\n",
        "        # Find all the faces and face enqcodings in the frame of video\n",
        "        face_locations = face_recognition.face_locations(frame)\n",
        "        #face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
        "        face_landmarks_list = face_recognition.face_landmarks(frame)\n",
        "\n",
        "        # Loop through each face in this frame of video\n",
        "        for (top, right, bottom, left), face_landmarks in zip(face_locations, face_landmarks_list):\n",
        "\n",
        "            name=\"Unknown\"\n",
        "            \n",
        "            # Draw a box around the face\n",
        "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
        "            \n",
        "        \n",
        "            \n",
        "            # Draw a label with a name below the face\n",
        "            cv2.rectangle(frame, (left, bottom), (right, bottom + 35), (0, 0, 255), cv2.FILLED)\n",
        "            font = cv2.FONT_HERSHEY_DUPLEX\n",
        "            cv2.putText(frame, name, (left + 6, bottom + 25), font, 1.0, (255, 255, 255), 1)\n",
        "\n",
        "\n",
        "            # Display text for mouth open / close\n",
        "            ret_mouth_open = is_mouth_open(face_landmarks)\n",
        "            if ret_mouth_open is True:\n",
        "                text = 'Mouth is open'\n",
        "            else:\n",
        "                text = 'Open your mouth'\n",
        "            cv2.putText(frame, text, (left, top - 50), cv2.FONT_HERSHEY_DUPLEX, 1.0, (255, 255, 255), 1)\n",
        "\n",
        "\n",
        "        # Display the resulting image\n",
        "        cv2.imshow('Video', frame)\n",
        "\n",
        "        # Hit 'q' on the keyboard to quit!\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    # Release handle to the webcam\n",
        "    video_capture.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P42SSxaFXk4_"
      },
      "source": [
        "#Face bluring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UHy5Z2sXdLB"
      },
      "source": [
        "# blur all faces\n",
        "def face_blur(filename):\n",
        "    video=[]\n",
        "    video_capture = cv2.VideoCapture(filename)\n",
        "    total_len = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    # Get a reference to webcam #0 (the default one)\n",
        "    size=(int(video_capture.get(3)),int(video_capture.get(4)))\n",
        "    i=1\n",
        "    video_name=filename.split(\".\")[0]+\".avi\"\n",
        "    print(\"\\n\"+video_name)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(video_name,fourcc, 24.0, size)\n",
        "\n",
        "    while True:\n",
        "        # Grab a single frame of video\n",
        "        try:\n",
        "          ret, frame = video_capture.read()\n",
        "          #frame=cv2.cvtColor(frame,cv2.COLOR_RGB2GRAY)\n",
        "          frame=cv2.flip(frame, 1)\n",
        "          # Find all the faces and face enqcodings in the frame of video\n",
        "          face_locations = face_recognition.face_locations(frame)\n",
        "      \n",
        "          # Loop through each face in this frame of video\n",
        "          for location in face_locations:    \n",
        "              top, right, bottom, left=location\n",
        "              # Draw a box around the face\n",
        "              cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
        "              sub_face = frame[top:bottom, left:right]\n",
        "          \n",
        "              # apply a gaussian blur on this new recangle image\n",
        "              sub_face = cv2.GaussianBlur(sub_face,(23, 23), 30)\n",
        "              # merge this blurry rectangle to our final image\n",
        "              frame[top:top+sub_face.shape[0], left:left+sub_face.shape[1]] = sub_face\n",
        "          out.write(frame)\n",
        "          print(\"\\r Data is being processed %s/%s\"%(i,total_len),end=\"\")\n",
        "          i+=1\n",
        "        except Exception as e:\n",
        "          break\n",
        "    out.release()\n",
        "\n",
        "        \n",
        "\n",
        "    "
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE2xUX2aXeg_"
      },
      "source": [
        "#Motion tracker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-vRWbI3W_DA"
      },
      "source": [
        "# Tracks selected objects\n",
        "def motion_tracker_multiple(nb_of_obj,filename=\"\"):\n",
        "    # Set up tracker.\n",
        "    # Instead of KCF, you can also use\n",
        "    (major_ver,minor_ver,subminor_ver)=(cv2.__version__).split('.')\n",
        "    tracker_types = ['BOOSTING', 'MIL','KCF', 'TLD', 'MEDIANFLOW', 'GOTURN', 'MOSSE', 'CSRT']\n",
        "    tracker_type = tracker_types[2]\n",
        "    \n",
        "    trackers=[None]*nb_of_obj\n",
        "    for i in range(nb_of_obj):\n",
        "        if int(minor_ver) < 3:\n",
        "            trackers[i] = cv2.Tracker_create(tracker_type)\n",
        "        else:\n",
        "            if tracker_type == 'BOOSTING':\n",
        "                trackers[i] = cv2.TrackerBoosting_create()\n",
        "            if tracker_type == 'MIL':\n",
        "                trackers[i] = cv2.TrackerMIL_create()\n",
        "            if tracker_type == 'KCF':\n",
        "                trackers[i] = cv2.TrackerKCF_create()\n",
        "            if tracker_type == 'TLD':\n",
        "                trackers[i] = cv2.TrackerTLD_create()\n",
        "            if tracker_type == 'MEDIANFLOW':\n",
        "                trackers[i] = cv2.TrackerMedianFlow_create()\n",
        "            if tracker_type == 'GOTURN':\n",
        "                trackers[i] = cv2.TrackerGOTURN_create()\n",
        "            if tracker_type == 'MOSSE':\n",
        "                trackers[i] = cv2.TrackerMOSSE_create()\n",
        "            if tracker_type == \"CSRT\":\n",
        "                trackers[i] = cv2.TrackerCSRT_create()\n",
        "    \n",
        "    # Read video\n",
        "    if len(filename)==0:\n",
        "      video_capture = cv2.VideoCapture(2)\n",
        "    else :\n",
        "      video_capture = cv2.VideoCapture(filename)\n",
        "       #webcam (if not working try \"0\" or \"1\")\n",
        "    video_capture.set(3,640)\n",
        "    video_capture.set(4,480)\n",
        "    \n",
        "\n",
        "    # Read first frame.\n",
        "    ok, frame = video_capture.read()\n",
        "    #frame=cv2.cvtColor(frame,cv2.COLOR_RGB2GRAY)\n",
        "    frame=cv2.flip(frame, 1)\n",
        "    if not ok:\n",
        "        print('Cannot read video file')\n",
        "        sys.exit()\n",
        "        \n",
        "    # Initialize tracker with first frame and bounding box\n",
        "    ok_bboxs=[[None,None]]*nb_of_obj\n",
        "    palette = 255*np.asarray(sns.color_palette(None, nb_of_obj))\n",
        "    for i in range(nb_of_obj):\n",
        "        bbox = cv2.selectROI(frame, False)\n",
        "        ok = trackers[i].init(frame, bbox)\n",
        "        ok_bboxs[i]=[ok,bbox]\n",
        "    \n",
        "    while True:\n",
        "        # Read a new frame\n",
        "        ok, frame = video_capture.read()\n",
        "        frame=cv2.flip(frame, 1)\n",
        "        if not ok:\n",
        "            break\n",
        "            \n",
        "        # Start timer\n",
        "        timer = cv2.getTickCount()\n",
        "    \n",
        "       \n",
        "        # Calculate Frames per second (FPS)\n",
        "        fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
        "\n",
        "        for i in range(nb_of_obj):\n",
        "            # Update tracker\n",
        "            ok_bboxs[i] = trackers[i].update(frame)\n",
        "            # Draw bounding box\n",
        "            if ok_bboxs[i][0]:\n",
        "                # Tracking success\n",
        "                p1 = (int(ok_bboxs[i][1][0]), int(ok_bboxs[i][1][1]))\n",
        "                p2 = (int(ok_bboxs[i][1][0] + ok_bboxs[i][1][2]), int(ok_bboxs[i][1][1] + ok_bboxs[i][1][3]))\n",
        "                cv2.rectangle(frame, p1, p2, palette[i], 2, 1)\n",
        "                cv2.putText(frame, \"object \"+str(i), (p1[0] + 6, p2[1] + 25), cv2.FONT_HERSHEY_SIMPLEX, 1.0, palette[i], 1)\n",
        "\n",
        "            else :\n",
        "                # Tracking failure\n",
        "                cv2.putText(frame, \"Tracking failure detected for obj \"+\" \"*i+str(i), (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2)\n",
        "        \n",
        "            # Display tracker type on frame\n",
        "            cv2.putText(frame, tracker_type + \" Tracker\", (100,20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,170,50),2)\n",
        "            \n",
        "            # Display FPS on frame\n",
        "            cv2.putText(frame, \"FPS : \" + str(int(fps)), (100,50), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,170,50), 2)\n",
        "            # Display result\n",
        "            cv2.imshow(\"Tracking\", frame)\n",
        "        # Exit if ESC pressed\n",
        "        k = cv2.waitKey(1) & 0xff\n",
        "        if k == 27 : break"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJUe6cbKXYw5"
      },
      "source": [
        "#Motion sensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwQUb7AkWZZF"
      },
      "source": [
        "# Detects and tracks moving objects\n",
        "def motion_sensor_v2(filename=\"\"):\n",
        "\n",
        "    ### input type ###\n",
        "    if len(filename)==0:\n",
        "      video_capture = cv2.VideoCapture(2)\n",
        "    else :\n",
        "      video_capture = cv2.VideoCapture(filename)\n",
        "    \n",
        "    #background substraction\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
        "    fgbg = cv2.createBackgroundSubtractorKNN()\n",
        "    \n",
        "    firstFrame = None\n",
        "    initBB2 = None\n",
        "    fps = None\n",
        "    differ = None\n",
        "    now = ''\n",
        "    framecounter = 0\n",
        "    trackeron=0\n",
        "    min_area=400\n",
        "\n",
        "    tracked_items=[]\n",
        "\n",
        "    while True:\n",
        "        \n",
        "        ret, frame = video_capture.read()\n",
        "        thresh=frame\n",
        "        # resize the frame to 500\n",
        "        frame = cv2.resize(frame, (640, 480))\n",
        "        frame=frame[240:,:]  # optionnal (hide non unseful zones to speed up process)\n",
        "        \n",
        "        #cv2.normalize(frame, frame, -100, 155, cv2.NORM_MINMAX)\n",
        "        fgmask=frame\n",
        "        framecounter = framecounter+1\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        gray = cv2.GaussianBlur(gray, (21, 21), 0)\n",
        "\n",
        "        # if the first frame is None, initialize it\n",
        "        if firstFrame is None:\n",
        "            firstFrame = gray\n",
        "            continue\n",
        "        \n",
        "        fgmask = fgbg.apply(gray)\n",
        "        fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)\n",
        "        \n",
        "        # dilate the thresholded image to fill in holes, then find contours on thresholded image\n",
        "        thresh = cv2.dilate(fgmask, None, iterations=3)\n",
        "        \n",
        "        if framecounter>10:\n",
        "            cnts,hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            # loop over the contours identified\n",
        "            contourcount = 0\n",
        "            for c in cnts:\n",
        "                contourcount +=  1\n",
        "\n",
        "            # if the contour is too small, ignore it\n",
        "                (x, y, w, h) = cv2.boundingRect(c)\n",
        "                if w*h<min_area:\n",
        "                    continue\n",
        "                initBB2 =(x,y,w,h)\n",
        "        \n",
        "                #for Trackers :   0:tracker, 1:box, 2:ok, 3:ok count, 4:static frame\n",
        "            \n",
        "                nb_of_items=len(tracked_items)\n",
        "                dist=0\n",
        "\n",
        "                if nb_of_items==0:\n",
        "                    tracker=cv2.TrackerKCF_create()\n",
        "                    tracked_items.append([tracker,initBB2,tracker.init(frame,initBB2),framecounter,0])    \n",
        "                for track in tracked_items:\n",
        "                    if distance(track[1][:2],[x,y],40):\n",
        "                        dist+=1\n",
        "                if dist==nb_of_items:\n",
        "                    tracker=cv2.TrackerKCF_create()\n",
        "                    tracked_items.append([tracker,initBB2,tracker.init(frame,initBB2),framecounter,0])\n",
        "                    \n",
        "            for track in tracked_items:\n",
        "                previous_pos=track[1]\n",
        "                track[2],track[1]=track[0].update(frame)\n",
        "                if track[2]:\n",
        "                    track[3]+=1\n",
        "                if not distance(previous_pos[:2],track[1][:2],5):\n",
        "                    track[4]+=1\n",
        "                if framecounter-track[3]>30 or track[4]>=20:\n",
        "                    tracked_items.remove(track)\n",
        "            for track in tracked_items:\n",
        "                if track[2]:\n",
        "                    p1 = (int(track[1][0]), int(track[1][1]))\n",
        "                    p2 = (int(track[1][0] + track[1][2]), int(track[1][1] + track[1][3]))\n",
        "                    cv2.rectangle(frame, p1, p2, (0, 255, 0), 2, 1)\n",
        "\n",
        "        # show the frame \n",
        "        cv2.imshow(\"Video stream\", frame)\n",
        "\n",
        "        cv2.imshow(\"Video stream2\", thresh)\n",
        "        \n",
        "        key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "        # if the `q` key is pressed, break from the lop\n",
        "        if key == ord(\"q\"):\n",
        "            break\n",
        "        if key == ord(\"d\"):\n",
        "            firstFrame = None\n",
        "        lastframe = frame\n",
        "\n",
        "        k = cv2.waitKey(30) & 0xff\n",
        "        if k == 27:\n",
        "            break\n",
        "\n",
        "    video_capture.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "def distance(ptsA,ptsB,eps):\n",
        "    if len(ptsA)==0:\n",
        "        return True\n",
        "    else:\n",
        "        return np.sqrt((ptsA[0]-ptsB[0])**2+(ptsA[1]-ptsB[1])**2)>eps"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDVXqIc0XURI"
      },
      "source": [
        "#Human detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F9ry_k8XLVY"
      },
      "source": [
        "# NOT WORKING YET\n",
        "def human_detector(filename=\"\"):\n",
        "        # initialize the HOG descriptor/person detector\n",
        "    hog = cv2.HOGDescriptor()\n",
        "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
        "\n",
        "    #cv2.startWindowThread()\n",
        "\n",
        "    if len(filename)==0:\n",
        "      video_capture = cv2.VideoCapture(2)\n",
        "    else :\n",
        "      video_capture = cv2.VideoCapture(filename)\n",
        "\n",
        "    # the output will be written to output.avi\n",
        "    \n",
        "    while(True):\n",
        "        # Capture frame-by-frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # resizing for faster detection\n",
        "        frame = cv2.resize(frame, (640, 480))\n",
        "        cv2.normalize(frame, frame, -100, 155, cv2.NORM_MINMAX)\n",
        "        # using a greyscale picture, also for faster detection\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        # detect people in the image\n",
        "        # returns the bounding boxes for the detected objects\n",
        "        boxes, weights = hog.detectMultiScale(frame, winStride=(8,8) )\n",
        "\n",
        "        boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])\n",
        "\n",
        "        for (xA, yA, xB, yB) in boxes:\n",
        "            # display the detected boxes in the colour picture\n",
        "            cv2.rectangle(frame, (xA, yA), (xB, yB),(0, 255, 0), 2)\n",
        "        \n",
        "        # Display the resulting frame\n",
        "        cv2.imshow('frame',frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    # When everything done, release the capture\n",
        "    cap.release()\n",
        "    # finally, close the window\n",
        "    cv2.destroyAllWindows()\n",
        "    cv2.waitKey(1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II9_B8QrXOmi"
      },
      "source": [
        "# Command Example "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD2WmP9rXNqI"
      },
      "source": [
        "#human_detector()\n",
        "#motion_sensor_v2()\n",
        "#motion_tracker_multiple(2)\n",
        "#mouth_open_close()\n",
        "face_blur(\"\")   #  enter filename here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKmOjbu7vRWz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}