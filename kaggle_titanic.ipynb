{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle_titanic.ipynb",
      "provenance": [],
      "mount_file_id": "1f2lsES3_7exnQeqgcSTZeJPdj0GBBrhC",
      "authorship_tag": "ABX9TyPA1vQ/OWwMaSB55LcPydRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomJiX/ML_start/blob/main/kaggle_titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fe76d1d1ded592430e7548feacfa38dc42f085d9",
        "id": "HYXj3ibXkclN"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "jxKOY7c3kclR"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "from keras.utils import to_categorical, normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "import gc"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cX3wCBMmVj3"
      },
      "source": [
        "#Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M237Gtv6ddJu"
      },
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c titanic\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0T6wag99wJm"
      },
      "source": [
        "DATA=\"/content/train.csv\"\n",
        "TEST=\"/content/test.csv\"\n",
        "\n",
        "CHALLENGE_NAME=\"titanic\"\n",
        "\n",
        "# Load dataset.\n",
        "dftrain = pd.read_csv(DATA)\n",
        "dfeval = pd.read_csv(TEST)\n",
        "\n",
        "dftrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5uAJH8m9wJp"
      },
      "source": [
        "Select usefull parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9saeSUKtDui"
      },
      "source": [
        "def clean_data_add_features(data):\n",
        "    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n",
        "    data['IsAlone']=(data['FamilySize']==1).astype(int)\n",
        "    data=pd.get_dummies(data, columns=[\"Pclass\",\"Sex\"])\n",
        "    data=data.fillna(data.mean())\n",
        "    data[\"Age\"]=(data[\"Age\"]-data[\"Age\"].min())/(data[\"Age\"].max()-data[\"Age\"].min())\n",
        "    return data\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43bnA_tY9wJp"
      },
      "source": [
        "useful_columns=[\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Survived\"]\n",
        "data_set=pd.read_csv(DATA,usecols=useful_columns)\n",
        "data_set=clean_data_add_features(data_set)\n",
        "\n",
        "sub_set=pd.read_csv(TEST,usecols=[\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\"])\n",
        "sub_set=clean_data_add_features(sub_set)\n",
        "   \n",
        "\n",
        "labels_df = np.array(data_set['Survived'])# Remove the labels from the features\n",
        "# axis 1 refers to the columns\n",
        "features_df= data_set.drop('Survived', axis = 1)\n",
        "# Saving feature names for later use\n",
        "feature_list = list(features_df.columns)# Convert to numpy array\n",
        "features = np.array(features_df)\n",
        "submission = np.array(sub_set)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdFQPj0geHkg"
      },
      "source": [
        "#Dataset data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX4X-htyeIRK"
      },
      "source": [
        "DATA_SIZE=np.shape(features)\n",
        "print(DATA_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOytJHCBdtax"
      },
      "source": [
        "#Model params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "9aa032f0f6da539d23918890d2d131cc3aac8c7a",
        "id": "C_0rre5nkcml"
      },
      "source": [
        "DENSE_LAYERS=list(range(1,7)) #6\n",
        "LAYER_SIZE=[16,32,64]  #64\n",
        "FAST_RUN=False\n",
        "print(\"{} Number of models\".format(len(DENSE_LAYERS)*len(LAYER_SIZE)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdHK929Rnz05"
      },
      "source": [
        "%rm -rf *-logs"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "8c9f833c1441b657c779844912d0b8028218d454",
        "id": "qkwE5g36kcme"
      },
      "source": [
        "%%capture\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "earlystop = EarlyStopping(patience=10)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
        "                        patience=2, \n",
        "                        verbose=1, \n",
        "                        factor=0.5, \n",
        "                        min_lr=0.00001)\n",
        "\n",
        "EPOCHS=3 if FAST_RUN else 30\n",
        "for dense_layer in DENSE_LAYERS:\n",
        "        for layer_size in LAYER_SIZE:\n",
        "            NAME = \"{}-{}-dense-drop-{}\".format(layer_size,dense_layer, int(time.time()))\n",
        "            print(NAME)\n",
        "\n",
        "            model = Sequential()\n",
        "\n",
        "            model.add(Flatten(input_shape=(DATA_SIZE[1],)))\n",
        "\n",
        "            for _ in range(dense_layer):\n",
        "                model.add(Dense(layer_size,activation=\"relu\"))\n",
        "                #model.add(Dropout(0.25))\n",
        "\n",
        "            model.add(Dense(1, activation='sigmoid')) # 2 because we have survive or not\n",
        "\n",
        "            tensorboard = TensorBoard(log_dir=\"{}-logs19_11/{}\".format(CHALLENGE_NAME,NAME))\n",
        "            callbacks = [earlystop, learning_rate_reduction, tensorboard]\n",
        "            model.compile(loss='binary_crossentropy',\n",
        "                        optimizer='adam',\n",
        "                        metrics=['accuracy'],\n",
        "                        )\n",
        "            model.fit(features, labels_df,\n",
        "                    batch_size=32,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_split=0.3,\n",
        "                    callbacks=callbacks)\n",
        "            gc.collect()\n",
        "\n",
        "%cp -R /content/*-logs* /content/drive/My\\ Drive/Colab\\ Notebooks/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6bSG2LTtQ5I"
      },
      "source": [
        "# Particular NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfDNO_LptRUF"
      },
      "source": [
        "DENSE_LAYERS=5 #5\n",
        "LAYER_SIZE=64  #64\n",
        "FAST_RUN=False\n",
        "\n",
        "#print(\"{} Number of models\".format())"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibx1tSpMtVX0"
      },
      "source": [
        "%%capture\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization,Input\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop, Adamax, Adagrad, Adam, Nadam, SGD\n",
        "earlystop = EarlyStopping(patience=10)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
        "                        patience=2, \n",
        "                        verbose=1, \n",
        "                        factor=0.5, \n",
        "                        min_lr=0.00001)\n",
        "callbacks = [earlystop, learning_rate_reduction]\n",
        "EPOCHS=3 if FAST_RUN else 30\n",
        "def create_model():\n",
        "    \n",
        "\n",
        "    NAME = \"{}-{}-dense-{}\".format(LAYER_SIZE,DENSE_LAYERS, int(time.time()))\n",
        "    print(NAME)\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "\n",
        "    model.add(Input(shape=DATA_SIZE[1]))\n",
        "\n",
        "    for _ in range(DENSE_LAYERS):\n",
        "        model.add(Dense(LAYER_SIZE,activation=\"relu\"))\n",
        "        #model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "    \n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "NN_model=create_model()\n",
        "history=NN_model.fit(features, labels_df,\n",
        "        batch_size=32,\n",
        "        epochs=EPOCHS,\n",
        "        validation_split=0.3,\n",
        "        callbacks=callbacks)\n",
        "gc.collect()\n",
        "NN_model_acc=history.history['accuracy'][-1]"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmIonqSUztid"
      },
      "source": [
        "NN_model_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-chDfzx8O3v"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
        "ax1.plot(history.history['loss'], color='b', label=\"Training loss\")\n",
        "ax1.plot(history.history['val_loss'], color='r', label=\"validation loss\")\n",
        "ax1.set_xticks(np.arange(1, EPOCHS, 1))\n",
        "ax1.set_yticks(np.arange(0, 1, 0.1))\n",
        "\n",
        "ax2.plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
        "ax2.plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
        "ax2.set_xticks(np.arange(1, EPOCHS, 1))\n",
        "\n",
        "legend = plt.legend(loc='best', shadow=True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WLl_djb3Gxi"
      },
      "source": [
        "%%capture\n",
        "def create_model2(input_shape=DATA_SIZE[1],\n",
        "                number_hidden=4, \n",
        "                neurons_per_hidden=32,\n",
        "                hidden_drop_rate= 0.2,\n",
        "                hidden_activation = 'selu',\n",
        "                hidden_initializer=\"lecun_normal\",\n",
        "                output_activation ='sigmoid',\n",
        "                loss='binary_crossentropy',\n",
        "                optimizer = Nadam(lr=0.0005),\n",
        "                ):\n",
        "    \n",
        "    #create model\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=input_shape)),\n",
        "    for layer in range(number_hidden):\n",
        "        model.add(Dense(neurons_per_hidden, activation = hidden_activation ,kernel_initializer=hidden_initializer))\n",
        "        #model.add(Dropout(hidden_drop_rate))\n",
        "    model.add(Dense(1, activation = output_activation))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=loss, \n",
        "                  #optimizer = Nadam(lr=lr), \n",
        "                  optimizer = Nadam(lr=0.0005),\n",
        "                  metrics = ['accuracy'])\n",
        "    return model\n",
        "NN_model2=create_model2()\n",
        "history=NN_model2.fit(features, labels_df,\n",
        "        batch_size=32,\n",
        "        epochs=EPOCHS,\n",
        "        validation_split=0.3,\n",
        "        callbacks=callbacks)\n",
        "gc.collect()\n",
        "NN_model_acc2=history.history['accuracy'][-1]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuV_U8MD3HC9"
      },
      "source": [
        "other model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVpH0ljLK4U3"
      },
      "source": [
        "Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISPlULEdK3sO"
      },
      "source": [
        "%%capture\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "n_split=10\n",
        "\n",
        "cv1=[]\n",
        "cv2=[]\n",
        "for train_index,test_index in KFold(n_split).split(features):\n",
        "    x_train,x_test=features[train_index],features[test_index]\n",
        "    y_train,y_test=labels_df[train_index],labels_df[test_index]\n",
        "    \n",
        "    model1=create_model()\n",
        "    model1.fit(x_train, y_train,epochs=20)\n",
        "    cv1.append(model1.evaluate(x_test,y_test)[1])\n",
        "    model2=create_model()\n",
        "    model2.fit(x_train, y_train,epochs=20)\n",
        "    cv2.append(model2.evaluate(x_test,y_test)[1])\n",
        "\n",
        "cv_data=pd.DataFrame({'CV Mean':[np.mean(cv1),np.mean(cv2)],'Std':[np.std(cv1),np.std(cv2)],\"Model_acc\":[NN_model_acc,NN_model_acc2]},index=['NN_model','NN_model2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STrNhw81mSKd"
      },
      "source": [
        "cv_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXIQzct88g6Q"
      },
      "source": [
        "#Trying out different model2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-xQp_F48grP"
      },
      "source": [
        "DENSE_LAYERS=list(range(1,7)) #6\n",
        "LAYER_SIZE=[10,16,32,64]  #64\n",
        "FAST_RUN=False\n",
        "print(\"{} Number of models\".format(len(DENSE_LAYERS)*len(LAYER_SIZE)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIPkmIO79_Bc"
      },
      "source": [
        "%rm -rf *-logs*"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8IgMNJx80I5"
      },
      "source": [
        "%%capture\n",
        "EPOCHS=3 if FAST_RUN else 30\n",
        "for dense_layer in DENSE_LAYERS:\n",
        "        for layer_size in LAYER_SIZE:\n",
        "            NAME = \"{}-{}-dense-{}\".format(layer_size,dense_layer, int(time.time()))\n",
        "            print(NAME)\n",
        "\n",
        "            tensorboard = TensorBoard(log_dir=\"{}-logs19_11-m2/{}\".format(CHALLENGE_NAME,NAME))\n",
        "            callbacks = [earlystop, learning_rate_reduction, tensorboard]\n",
        "            model=create_model2(number_hidden=dense_layer,neurons_per_hidden=layer_size)\n",
        "          \n",
        "            model.fit(features, labels_df,\n",
        "                    batch_size=32,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_split=0.3,\n",
        "                    callbacks=callbacks)\n",
        "            gc.collect()\n",
        "\n",
        "%cp -R /content/*-logs* /content/drive/My\\ Drive/Colab\\ Notebooks/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl1EPQkCim7a"
      },
      "source": [
        "#Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ygbzam-ipOz"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/titanic-logs19_11-m2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg98MFo8tJVK"
      },
      "source": [
        "#Non NN models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuhAiPfZ3rJW"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression #logistic regression\n",
        "from sklearn import svm #support vector Machine\n",
        "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
        "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
        "from sklearn.naive_bayes import GaussianNB #Naive bayes\n",
        "from sklearn.tree import DecisionTreeClassifier #Decision Tree\n",
        "from sklearn.model_selection import train_test_split #training and testing data split\n",
        "from sklearn import metrics #accuracy measure\n",
        "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
        "import xgboost as xgb"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLAClMLvuGwL"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels_df, test_size = 0.25, random_state = 42)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpsX-3OZKIvy"
      },
      "source": [
        "Try and Plot different classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o35Dj7A_1BnY"
      },
      "source": [
        "from sklearn.model_selection import KFold #for K-fold cross validation\n",
        "from sklearn.model_selection import cross_val_score #score evaluation\n",
        "from sklearn.model_selection import cross_val_predict #prediction\n",
        "kfold = KFold(n_splits=3, shuffle=True) # k=10, split the data into 10 equal parts\n",
        "xyz=[]\n",
        "accuracy=[]\n",
        "std=[]\n",
        "model_accuracy=[]\n",
        "trained_models=[]\n",
        "classifiers=['Linear Svm','Radial Svm','Decision Tree','Random Forest','Naive Bayes','Logistic Regression','KNN','XGBoost']\n",
        "models=[svm.SVC(kernel='linear',C=0.1,gamma=0.1),svm.SVC(kernel='rbf',C=0.1,gamma=0.1),DecisionTreeClassifier(),RandomForestClassifier(n_estimators=100),GaussianNB(),LogisticRegression(),KNeighborsClassifier(),xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)]\n",
        "j=0\n",
        "total=len(classifiers)\n",
        "for i in models:\n",
        "    model = i\n",
        "    print(\"\\r{} training... {}/{} Trained\".format(classifiers[j],j,total),end=\"\")\n",
        "    j+=1\n",
        "    model.fit(train_features,train_labels)\n",
        "    trained_models.append(model)\n",
        "    model_accuracy.append(metrics.accuracy_score(model.predict(test_features),test_labels))\n",
        "    cv_result = cross_val_score(model,features_df,labels_df, cv = kfold,scoring = \"accuracy\")\n",
        "    cv_result=cv_result\n",
        "    xyz.append(cv_result.mean())\n",
        "    std.append(cv_result.std())\n",
        "    accuracy.append(cv_result)\n",
        "\n",
        "model_df=pd.DataFrame({'models':trained_models},index=classifiers)\n",
        "model_df=pd.concat([model_df,pd.DataFrame({'models':[NN_model,NN_model2]}, index=[\"NN_model\",\"NN_model2\"])])\n",
        "new_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std,\"Model_acc\":model_accuracy},index=classifiers)\n",
        "new_models_dataframe2=pd.concat([new_models_dataframe2,cv_data])\n",
        "new_models_dataframe2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoECAaSFOpVi"
      },
      "source": [
        "print('Model with best accuracy : {}'.format(new_models_dataframe2['Model_acc'].idxmax(1)))\n",
        "print('Model with best Cross validation mean : {}'.format(new_models_dataframe2['CV Mean'].idxmax(1)))\n",
        "\n",
        "b_mod_name=(new_models_dataframe2['Model_acc']*1.1+new_models_dataframe2['CV Mean']*0.9).idxmax(1)\n",
        "print('Model to choose : {}'.format(b_mod_name))\n",
        "best_model=model_df['models'][b_mod_name]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYbfAOaZtNmy"
      },
      "source": [
        "#Particular fit , bagging and submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs34WfoitUJW"
      },
      "source": [
        "ITER=10\n",
        "CLASSIFIER= b_mod_name\n",
        "print(CLASSIFIER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnC5SLbttNNv"
      },
      "source": [
        "%%capture\n",
        "\n",
        "stack_predic=pd.DataFrame()\n",
        "for i in range(ITER):\n",
        "    shuffle = np.array(sorted(np.c_[train_features,train_labels], key=lambda k: random.random()))\n",
        "    if CLASSIFIER == \"NN_model\":\n",
        "        model=create_model()\n",
        "        model.fit(np.array([x[:-1] for x in shuffle]),np.array([x[-1] for x in shuffle]),epochs=200)\n",
        "        stack_predic[CLASSIFIER+str(i)]=model.predict(submission).flatten()\n",
        "    if CLASSIFIER == \"NN_model2\":\n",
        "        model=create_model2()\n",
        "        model.fit(np.array([x[:-1] for x in shuffle]),np.array([x[-1] for x in shuffle]),epochs=200)\n",
        "        stack_predic[CLASSIFIER+str(i)]=model.predict(submission).flatten()\n",
        "    else:\n",
        "        model = models[classifiers.index(CLASSIFIER)]\n",
        "        model.fit(np.array([x[:-1] for x in shuffle]),np.array([x[-1] for x in shuffle]))\n",
        "        stack_predic[CLASSIFIER+str(i)]=model.predict(submission)\n",
        "        \n",
        "\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM40hE12n_5_"
      },
      "source": [
        "stack_predic.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiSKQN1WnKyH"
      },
      "source": [
        "sub_csv=pd.read_csv(TEST,usecols=['PassengerId'])\n",
        "sub_csv[\"Survived\"]=np.round(stack_predic.mean(1).to_numpy()).astype(int)\n",
        "sub_csv.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp8SSYoEbXqP"
      },
      "source": [
        "sub_csv.to_csv('submission.csv', index=False)\n",
        "#!kaggle competitions submit -c titanic -f submission.csv -m \"NN2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbrYnyS6ucSM"
      },
      "source": [
        ""
      ],
      "execution_count": 127,
      "outputs": []
    }
  ]
}